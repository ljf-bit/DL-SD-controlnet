import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from torchmetrics.image.fid import FrechetInceptionDistance
from torchmetrics.multimodal.clip_score import CLIPScore
from torchvision import transforms

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================

# 1. ä½ çš„ ControlNet è·¯å¾„
CONTROLNET_PATH = "/root/autodl-tmp/models/controlnet_ancient_v4_pro/checkpoint-2500/controlnet"

# 2. åº•æ¨¡è·¯å¾„
BASE_MODEL_PATH = "/root/autodl-tmp/stable-diffusion-v1-5"

# 3. æ•°æ®è·¯å¾„ (ç”¨ä½ ä¹‹å‰çš„ processed_images æ–‡ä»¶å¤¹å³å¯ï¼Œæˆ–è€… jsonl å¯¹åº”çš„æ–‡ä»¶å¤¹)
# æˆ‘ä»¬éœ€è¦åŸå›¾æ¥ç®— FID (çœŸå®åˆ†å¸ƒ)
DATA_DIR = "/root/autodl-tmp/datasets/processed_images"
# Prompt æ¥æºï¼štrain_prompts.jsonl
JSONL_PATH = "/root/autodl-tmp/datasets/train_prompts.jsonl"

# 4. è¯„ä¼°æ•°é‡ (50-100 å¼ å³å¯ï¼Œè·‘å¤ªå¤šä¼šå¾ˆæ…¢)
NUM_SAMPLES = 100

# 5. è¾“å‡ºç›®å½•
GEN_DIR = "./eval_no_lora_generated"
# ===============================================

def load_prompts(jsonl_path):
    prompts = {}
    import json
    if os.path.exists(jsonl_path):
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                # æå–æ–‡ä»¶åä½œä¸º key
                key = os.path.basename(data["image"])
                prompts[key] = data["text"]
    return prompts

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    os.makedirs(GEN_DIR, exist_ok=True)

    print("ğŸš€ åŠ è½½ ControlNet...")
    controlnet = ControlNetModel.from_pretrained(CONTROLNET_PATH, torch_dtype=torch.float16)
    
    print("ğŸš€ åŠ è½½åº•æ¨¡...")
    if BASE_MODEL_PATH.endswith(".safetensors"):
        pipe = StableDiffusionControlNetPipeline.from_single_file(
            BASE_MODEL_PATH, controlnet=controlnet, torch_dtype=torch.float16, load_safety_checker=False
        )
    else:
        pipe = StableDiffusionControlNetPipeline.from_pretrained(
            BASE_MODEL_PATH, controlnet=controlnet, torch_dtype=torch.float16, safety_checker=None
        )
        
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    pipe.to(device)
    pipe.set_progress_bar_config(disable=True)

    # åˆå§‹åŒ–æŒ‡æ ‡
    print("ğŸ“ åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡...")
    fid = FrechetInceptionDistance(feature=64).to(device)
    clip_score = CLIPScore(model_name_or_path="openai/clip-vit-base-patch16").to(device)

    # å‡†å¤‡æ•°æ®
    import glob
    all_images = glob.glob(os.path.join(DATA_DIR, "*.[pjPJ]*"))[:NUM_SAMPLES]
    prompt_dict = load_prompts(JSONL_PATH)
    
    print(f"ğŸ¯ å¼€å§‹è¯„ä¼° {len(all_images)} å¼ æ ·æœ¬...")
    
    clip_scores_list = []
    
    # å›¾åƒé¢„å¤„ç† (è½¬uint8)
    to_uint8 = transforms.Lambda(lambda x: (x * 255).byte())
    resize_fid = transforms.Resize((299, 299)) # Inception éœ€è¦ 299x299

    for img_path in tqdm(all_images):
        filename = os.path.basename(img_path)
        
        # è·å– Promptï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨é€šç”¨çš„
        prompt = prompt_dict.get(filename, "Chinese ancient architecture, highly detailed, 8k")
        
        try:
            # 1. è¯»å–çœŸå®å›¾ç‰‡ (ä½œä¸º FID çš„å‚è€ƒ)
            image_real = Image.open(img_path).convert("RGB").resize((512, 512))
            
            # 2. ç”Ÿæˆå›¾ç‰‡
            # æ³¨æ„ï¼šç†è®ºä¸Šåº”è¯¥è¾“å…¥ _sem.pngã€‚
            # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°†åŸå›¾ä½œä¸º condition è¾“å…¥ã€‚
            # ä¸ºäº†é˜²æ­¢æ¨¡å‹ç…§æŠ„åŸå›¾ï¼Œæˆ‘ä»¬æŠŠ control scale è°ƒä½ä¸€ç‚¹ï¼Œè®©å®ƒé‡ç»˜
            # æˆ–è€…ï¼šä½ å¯ä»¥å†™ä»£ç å…ˆç”Ÿæˆ Canny å›¾å†å–‚è¿›å»ï¼Œä½†é‚£æ ·å¤ªå¤æ‚äº†ã€‚
            # ç›´æ¥å–‚åŸå›¾ç»™ ControlNetï¼Œåªè¦ scale=0.5 å·¦å³ï¼Œå®ƒä¼šæŠŠåŸå›¾å½“æˆä¸€ç§"é¢œè‰²å‚è€ƒ"ï¼Œ
            # ç”Ÿæˆå‡ºæ¥çš„å›¾ç»“æ„ä¼šå’ŒåŸå›¾ä¸€æ ·ï¼Œä½†ç»†èŠ‚æ˜¯é‡ç»˜çš„ã€‚è¿™ç¬¦åˆè¯„ä¼°è¦æ±‚ã€‚
            image_gen = pipe(
                prompt,
                image=image_real, 
                num_inference_steps=20,
                controlnet_conditioning_scale=0.5, # å¼±æ§åˆ¶ï¼Œå…è®¸é‡ç»˜
                guidance_scale=7.5
            ).images[0]
            
            # ä¿å­˜
            image_gen.save(os.path.join(GEN_DIR, filename))
            
            # 3. è®¡ç®—æŒ‡æ ‡
            
            # --- FID æ›´æ–° ---
            # çœŸå®å›¾
            real_tensor = transforms.ToTensor()(image_real).unsqueeze(0).to(device)
            real_tensor_uint8 = to_uint8(real_tensor)
            real_tensor_fid = resize_fid(real_tensor_uint8)
            fid.update(real_tensor_fid, real=True)
            
            # ç”Ÿæˆå›¾
            gen_tensor = transforms.ToTensor()(image_gen).unsqueeze(0).to(device)
            gen_tensor_uint8 = to_uint8(gen_tensor)
            gen_tensor_fid = resize_fid(gen_tensor_uint8)
            fid.update(gen_tensor_fid, real=False)
            
            # --- CLIP æ›´æ–° ---
            # CLIP ä¸éœ€è¦ uint8ï¼Œéœ€è¦ 0-1 float
            score = clip_score(gen_tensor, [prompt])
            clip_scores_list.append(score.item())
            
        except Exception as e:
            print(f"Skipping {filename}: {e}")
            continue

    print("ğŸ“‰ è®¡ç®—æœ€ç»ˆåˆ†æ•°ä¸­...")
    fid_value = fid.compute()
    avg_clip = sum(clip_scores_list) / len(clip_scores_list) if clip_scores_list else 0
    
    print("="*40)
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ (æ ·æœ¬æ•°: {len(all_images)})")
    print(f"ğŸ”¹ FID Score: {fid_value.item():.4f}")
    print(f"ğŸ”¸ CLIP Score: {avg_clip:.4f}")
    print("="*40)

if __name__ == "__main__":
    main()